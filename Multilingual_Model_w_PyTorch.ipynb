{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Multilingual Model w/ PyTorch",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhirslab/aiml-hotspot/blob/main/Multilingual_Model_w_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T00:20:35.41623Z",
          "iopub.execute_input": "2024-11-22T00:20:35.416629Z",
          "iopub.status.idle": "2024-11-22T00:20:47.037725Z",
          "shell.execute_reply.started": "2024-11-22T00:20:35.416591Z",
          "shell.execute_reply": "2024-11-22T00:20:47.036367Z"
        },
        "id": "GFcF2VLDO8YO",
        "outputId": "dfc2c173-c28d-453b-dca9-495138325e9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "/*\n",
        "    @author : Sudhir R. Pradhan\n",
        "*/\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Load the pre-trained mBERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Example of multilingual input (English, Spanish, and Hindi)\n",
        "# texts = [\n",
        "#     \"I love programming and learning new things.\",\n",
        "#     \"Me encanta programar y aprender cosas nuevas.\",\n",
        "#     \"मुझे प्रोग्रामिंग पसंद है और नई चीजें सीखना।\"\n",
        "# ]\n",
        "\n",
        "texts = [\n",
        "    \"I love programming and learning new \",\n",
        "    \"Me encanta programar y aprender cosas \",\n",
        "    \"मुझे प्रोग्रामिंग पसंद है और नई चीजें सीखना \"\n",
        "]\n",
        "\n",
        "# Tokenize the texts in multiple languages\n",
        "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Perform a masked language modeling task (predict the masked token)\n",
        "inputs['labels'] = inputs.input_ids.detach().clone()  # Copy input ids for training task\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Get the prediction logits and the token ids of the words in the input\n",
        "logits = outputs.logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Decode the predicted tokens back into words\n",
        "predicted_texts = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Input texts: \", texts)\n",
        "print(\"Predicted texts: \", predicted_texts)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T05:03:36.204944Z",
          "iopub.execute_input": "2024-11-21T05:03:36.205442Z",
          "iopub.status.idle": "2024-11-21T05:03:38.518806Z",
          "shell.execute_reply.started": "2024-11-21T05:03:36.205402Z",
          "shell.execute_reply": "2024-11-21T05:03:38.517446Z"
        },
        "id": "Tz01XPToO8YP",
        "outputId": "e2d84f1c-a46d-4e91-f7a8-1f78de9fee6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Input texts:  ['I love programming and learning new ', 'Me encanta programar y aprender cosas ', 'मुझे प्रोग्रामिंग पसंद है और नई चीजें सीखना ']\nPredicted texts:  ['. I love programming and learning new. learning new I I I I I. I love learning programming learning learning new learning', '. Me encanta programar y aprender cosas... Me Me Me Me Me Me aprender a programa, para', '. मुझे प्रोग्रामिंग पसंद है और नई चीजें सीखना ।']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> The warning you're seeing is a known and harmless issue related to the specific model architecture you're using. The BertForMaskedLM model from Hugging Face is initialized from a pre-trained bert-base-multilingual-cased model, which includes layers and parameters used in tasks like sentence classification (e.g., cls.seq_relationship and bert.pooler). These layers aren't necessary for the Masked Language Modeling (MLM) task and are therefore ignored when initializing BertForMaskedLM. This behavior is normal and can be ignored for this type of task.However, the main issue you're facing is related to the incorrect predictions from the model. The predictions you're getting (e.g., \"I I I I I love learning programming\") suggest that the model isn't predicting the masked token correctly. This could happen for a couple of reasons:1. Using BertForMaskedLM with Incorrect Inputs:The problem likely arises because of how the model is handling the <mask> token. It seems the model isn't understanding the task properly or generating random predictions based on the context. This could be because:The masked token is at the end of the sentence.The model doesn't have a good understanding of how to fill in the masked token in a multilingual context.2. Model Inference Strategy:The BertForMaskedLM model works by predicting the masked token based on the context surrounding it. When the model generates multiple words that don't make sense or are irrelevant, it's typically due to improper handling of the masked token or a context mismatch.Let's break it down step by step and make some refinements to fix the issue.\n"
      ],
      "metadata": {
        "id": "kJHKeAdiO8YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @author : Sudhir R. Pradhan\n",
        "\n",
        "# Install the necessary libraries\n",
        "# !pip install transformers torch\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Load the pre-trained mBERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Example of multilingual input (English, Spanish, and Hindi) with masked tokens\n",
        "texts = [\n",
        "    \"I love programming and learning new [MASK].\",\n",
        "    \"Me encanta programar y aprender cosas [MASK].\",\n",
        "    \"मुझे प्रोग्रामिंग पसंद है और नई चीजें सीखना [MASK]।\"\n",
        "]\n",
        "\n",
        "# Tokenize the texts in multiple languages\n",
        "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Perform a masked language modeling task (predict the masked token)\n",
        "# Prepare inputs for prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get the prediction logits for the masked token positions\n",
        "logits = outputs.logits\n",
        "\n",
        "# Extract the indices of the masked positions (index of the [MASK] token)\n",
        "masked_indices = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "# For each sentence, predict the masked token\n",
        "predicted_ids = []\n",
        "for idx in masked_indices:\n",
        "    predicted_token_id = logits[0, idx].argmax().item()\n",
        "    predicted_ids.append(predicted_token_id)\n",
        "\n",
        "# Decode the predicted tokens back into words\n",
        "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_ids)\n",
        "predicted_texts = []\n",
        "\n",
        "# Replace the [MASK] token with the predicted token in the original sentence\n",
        "for i, text in enumerate(texts):\n",
        "    predicted_text = text.replace('[MASK]', predicted_tokens[i])\n",
        "    predicted_texts.append(predicted_text)\n",
        "\n",
        "print(\"Input texts: \", texts)\n",
        "print(\"Predicted texts: \", predicted_texts)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T05:06:29.664597Z",
          "iopub.execute_input": "2024-11-21T05:06:29.665578Z",
          "iopub.status.idle": "2024-11-21T05:06:31.823716Z",
          "shell.execute_reply.started": "2024-11-21T05:06:29.665512Z",
          "shell.execute_reply": "2024-11-21T05:06:31.822517Z"
        },
        "id": "kupYuuWIO8YR",
        "outputId": "a8b2c98d-f342-41a8-fd08-f4caaa84eacf"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Input texts:  ['I love programming and learning new [MASK].', 'Me encanta programar y aprender cosas [MASK].', 'मुझे प्रोग्रामिंग पसंद है और नई चीजें सीखना [MASK]।']\nPredicted texts:  ['I love programming and learning new things.', 'Me encanta programar y aprender cosas I.', 'मुझे प्रोग्रामिंग पसंद है और नई चीजें सीखना things।']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n"
      ],
      "metadata": {
        "id": "ijBHgvsTO8YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaForMaskedLM, XLMRobertaTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the XLM-RoBERTa tokenizer and model\n",
        "model_name = \"xlm-roberta-large\"  # A multilingual transformer model\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
        "model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Ensure that the model handles masking\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
        "\n",
        "# Input sentences in multiple languages\n",
        "texts = [\n",
        "    \"I love programming and learning new <mask>.\",  # English\n",
        "    \"Me encanta programar y aprender nuevas <mask>.\",  # Spanish\n",
        "    \"मुझे प्रोग्रामिंग पसंद है और नई चीजें सीखना <mask>।\",  # Hindi\n",
        "]\n",
        "\n",
        "# Function to predict the next word for a masked token\n",
        "def predict_next_word(text):\n",
        "    # Tokenize input sentence and prepare it for masked language modeling\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "\n",
        "    # Create attention mask (1 for real tokens, 0 for padding tokens)\n",
        "    attention_mask = torch.ones(input_ids.shape, device=input_ids.device)\n",
        "\n",
        "    # Get predictions for the <mask> token\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Find the index of the <mask> token\n",
        "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1].item()\n",
        "\n",
        "    # Get the predicted token ID (highest probability for the <mask> token)\n",
        "    predicted_token_id = output.logits[0, mask_token_index].argmax().item()\n",
        "\n",
        "    # Decode the predicted token to the next word\n",
        "    predicted_word = tokenizer.decode(predicted_token_id, skip_special_tokens=True)\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Loop through the text inputs and predict the next word\n",
        "for text in texts:\n",
        "    next_word = predict_next_word(text)\n",
        "    print(f\"Input: {text}\")\n",
        "    print(f\"Predicted next word: {next_word}\\n\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T00:21:01.585104Z",
          "iopub.execute_input": "2024-11-22T00:21:01.585536Z",
          "iopub.status.idle": "2024-11-22T00:21:43.991651Z",
          "shell.execute_reply.started": "2024-11-22T00:21:01.585496Z",
          "shell.execute_reply": "2024-11-22T00:21:43.990525Z"
        },
        "id": "AlTc3t8EO8YS",
        "outputId": "a530fb1b-502d-4d90-f4dd-f5ab4476c4c4",
        "colab": {
          "referenced_widgets": [
            "f0853c6c918c49f898afe54c1e93d467",
            "ca5f6ceda9904c5f82722a5c288efe78",
            "2756605735b7421597ef8389dba0699a",
            "a61a1ce4f5624cafac64a1060d033452",
            "b81735732b904b5881bc6e592a4449de"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0853c6c918c49f898afe54c1e93d467"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca5f6ceda9904c5f82722a5c288efe78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2756605735b7421597ef8389dba0699a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a61a1ce4f5624cafac64a1060d033452"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b81735732b904b5881bc6e592a4449de"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Input: I love programming and learning new <mask>.\nPredicted next word: things\n\nInput: Me encanta programar y aprender nuevas <mask>.\nPredicted next word: cosas\n\nInput: मुझे प्रोग्रामिंग पसंद है और नई चीजें सीखना <mask>।\nPredicted next word: है\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}